# Stable Diffusion Inference Server

This directory contains a custom Stable Diffusion inference engine, built from scratch and deployed on [RunPod](https://www.runpod.io/). It exposes a Flask API for text-to-image generation using a pipeline of CLIP, VAE (Encoder/Decoder), and UNet models.

---

## ğŸ—ï¸ Directory Structure

```
StableDiffusion/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api.py           # Flask API server
â”‚   â”œâ”€â”€ outputs/         # Generated images (persisted)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ clip.py      # CLIP model
â”‚       â”œâ”€â”€ decoder.py   # VAE Decoder
â”‚       â”œâ”€â”€ encoder.py   # VAE Encoder
â”‚       â”œâ”€â”€ diffusion.py # UNet & diffusion process
â”‚       â”œâ”€â”€ model_loader.py
â”‚       â”œâ”€â”€ pipeline.py  # Generation pipeline
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ attention.py
â”‚           â”œâ”€â”€ ddpm.py
â”‚           â””â”€â”€ model_converter.py
â”‚
â”œâ”€â”€ weights/
â”‚   â”œâ”€â”€ v1-5-pruned-emaonly.ckpt  # Model weights
â”‚   â”œâ”€â”€ merges.txt                # Tokenizer merges (from HuggingFace)
â”‚   â””â”€â”€ vocab.json                # Tokenizer vocab (from HuggingFace)
```

---

## ğŸ§  Model Architecture

- **CLIP**: Used for prompt tokenization and text embedding ([OpenAI CLIP](https://github.com/openai/CLIP)).
- **VAE Encoder/Decoder**: Compresses and reconstructs images in latent space.
- **UNet**: Core of the diffusion process, denoising latent representations.
- **Custom Pipeline**: Orchestrates prompt processing, latent sampling, and image decoding.

All components are loaded from scratch using distilled weights and HuggingFace tokenizer files.

---

## ğŸ“¦ Weights & Tokenizer

- **Model Weights**: [v1-5-pruned-emaonly.ckpt](https://huggingface.co/stable-diffusion-v1-5/tree/main)
- **Tokenizer Files**: [merges.txt](https://huggingface.co/stable-diffusion-v1-5/tree/main/tokenizer), [vocab.json](https://huggingface.co/stable-diffusion-v1-5/tree/main/tokenizer)
- All files are stored in a persistent volume (`/runpod-volume/weights`) on RunPod for fast pod startup and reusability.

---

## ğŸš€ Deployment: RunPod Pod

- **Persistent Volume**: Model weights and tokenizer files are mounted at `/runpod-volume/weights` and outputs at `/runpod-volume/outputs`.
- **API Exposure**: Flask app runs on port `8000` and exposes `/inference` endpoint.
- **Pod Startup**: On pod start, all models and tokenizers are loaded into memory for fast inference.
- **Docker**: Use the provided `Dockerfile` for reproducible builds.

---

## ğŸ–¥ï¸ Running the API

### 1. With Flask (for development)

```bash
python app/api.py
```

### 2. With Gunicorn (for production)

```bash
gunicorn -w 1 -b 0.0.0.0:8000 app.api:app
```

### 3. With main.py Entrypoint (for RunPod)

Create a `main.py` with:

```python
from app.api import app

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8000)
```

Then run:
```bash
python main.py
```

---

## ğŸ”— API Usage Example

**POST** `/inference`

Request:
```json
{
  "uid": "<your-uid>",
  "prompt": "A futuristic cityscape at sunset",
  "steps": 50,
  "seed": 42,
  "strength": 0.9,
  "do_scale": true,
  "scale": 8.0
}
```

Response:
```json
{
  "message": "success",
  "uid": "abc12345",
  "image": "<base64-encoded-png>"
}
```

---

## ğŸ“ Logging & Debugging

- All requests and errors are logged to stdout (visible in RunPod pod logs).
- Output images are saved in `/runpod-volume/outputs` for persistence.
- For debugging, set `FLASK_DEBUG=true` in environment variables.

---

## ğŸ™ Acknowledgements

- [Umar Jamil](https://github.com/cloneofsimo) for the distilled Stable Diffusion implementation
- [HuggingFace](https://huggingface.co/stable-diffusion-v1-5) for model weights and tokenizer files
- [OpenAI CLIP](https://github.com/openai/CLIP)
- [RunPod](https://www.runpod.io/) for GPU pod hosting

---

## ğŸ“„ License

This project is for research and educational purposes. See the root `LICENSE` file for details.
